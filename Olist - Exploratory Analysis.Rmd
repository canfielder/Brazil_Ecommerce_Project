---
title: "Brazillian Ecommerce - EDA"
author: "Evan Canfield"
date: "3/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import
## Libraries
```{r}
if (!require(pacman)) {install.packages('pacman')} 
p_load(
    janitor
  , lubridate
  , Hmisc
  , skimr
  , tidyverse
)
```

## Data
```{r}
# Olist Dataset
df_cust <- read.csv(file = "data/olist_customers_dataset.csv", stringsAsFactors = FALSE)
df_geo_loc <- read.csv(file = "data/olist_geolocation_dataset.csv", stringsAsFactors = FALSE)
df_order_items <- read.csv(file = "data/olist_order_items_dataset.csv", stringsAsFactors = FALSE)
df_order_pay <- read.csv(file = "data/olist_order_payments_dataset.csv", stringsAsFactors = FALSE)
df_order_review <- read.csv(file = "data/olist_order_reviews_dataset.csv", stringsAsFactors = FALSE)
df_orders <- read.csv(file = "data/olist_orders_dataset.csv", stringsAsFactors = FALSE)
df_products <- read.csv(file = "data/olist_products_dataset.csv", stringsAsFactors = FALSE)
df_sellers <- read.csv(file = "data/olist_sellers_dataset.csv", stringsAsFactors = FALSE)
df_translation <- read.csv(file = "data/product_category_name_translation.csv", stringsAsFactors = FALSE)

# Brazillian Population Data
df_brazil_pop <- read.csv(file = "data/population_estimates_dou_2019.csv", stringsAsFactors = FALSE)
```

### Clean Names
The following function converts column names to snake case.
```{r}
clean_names_func <- function(df){
  df <- df %>% clean_names(case = "snake")
  return (df)
}

df_cust <- clean_names_func(df_cust)
df_geo_loc <- clean_names_func(df_geo_loc)
df_order_items <- clean_names_func(df_order_items)
df_order_pay <- clean_names_func(df_order_pay)
df_orders <- clean_names_func(df_orders)
df_products <- clean_names_func(df_products)
df_sellers <- clean_names_func(df_sellers)
df_translation <- clean_names_func(df_translation)
df_brazil_pop <- clean_names_func(df_brazil_pop)
```

# Inspect Data
```{r}
df_cust %>% glimpse()
```

```{r}
df_geo_loc %>% glimpse()
```

```{r}
df_order_items %>% glimpse()
```

```{r}
df_order_pay %>% glimpse()
```

```{r}
df_order_review %>% glimpse()
```

```{r}
df_orders %>% glimpse()
```

```{r}
df_products %>% glimpse()
```

```{r}
df_sellers %>% glimpse()
```

```{r}
df_translation %>% glimpse()
```

```{r}
df_brazil_pop %>% glimpse()
```

# Data Processing
There are several areas where we need to perform processing on the data before we can start to analyze. 

## Translate
We need to translate the prodcut category name from Protugese to English for greater understanding. The dataset provides a translation code dataset
```{r}
df_products_translate <- df_products %>% 
  left_join(y = df_translation,
            by = c("product_category_name" = "i_product_category_name")) %>% 
  select(product_id, product_category_name, product_category_name_english, everything()) %>% 
  select(-product_category_name)
```

Now that we've translated the data, we'll take a deeper look at the contents of the dataframe.
```{r}
#df_products_translate %>% describe()
```

## Product IDs - Shorten
The product ID tags are very long, and difficult to work with. For other id tags, this isn't a problem, because they will only be used for connecting tables. But the product id may be displayed on charts and tables. 

I wasn to see if I can shorten it, while maintaining the unique tag nature. There are 32,951 unique product id tags.
```{r}
prod_unique <- df_products_translate %>% 
  select(product_id) %>% 
  n_distinct()

prod_unique
```
The shortest product string in the product id column is 32 characters. So is the maximum. In actuality, all product id tags are 32 characters.
```{r}
min_prod_id_string <- min(nchar(df_products_translate$product_id))

paste0("The minimum length of a product id string is ", min_prod_id_string, " characters.")
```


If we shorten the tag, we need to determine that we still have unique ids. The following code crops the product id by **n** characters, from the right. It then checks against the known number of unique product ids. The process continues until enough characters have been cropped to create some duplicate id values. 
```{r}
n <- prod_unique
x = min_prod_id_string

while (TRUE){
  if (n == prod_unique){
    n <- df_products_translate %>% 
    select(product_id) %>% 
    mutate(product_id = str_sub(product_id, start = -x)) %>% 
    n_distinct()
    
    #print(paste0("Unique values: ",n))
    #print(paste0("Number of digits:", x))
    #print(paste0("***"))
    x = x-1
  }
    else{
      output_response <- paste0("Shortest string to maintain unique product_id: ", x + 2, " characters.")
      return(print(output_response))
    }
}
```

So, we can shorten product id to only 8 characters, a much more manageable string to view in tables and charts. We need to make this change at all product_id instances.
```{r}
df_products_translate <- df_products_translate %>% 
  mutate(product_id = str_sub(product_id, start = -x-2))

df_order_items <- df_order_items %>% 
  mutate(product_id = str_sub(product_id, start = -x-2))
```

 
## Shipping Times
shipping times will be an important variable for further analysis. We need to convert date-time values, which were imported as characters, into Posix date-time values. Once this is complete, we can then easily  

Let's take a quick look at the table.
```{r}
df_orders %>% 
  head()
```

Now lets look for missing data. Lucky for us there is no missing data in this table.
```{r}
df_orders %>%   skim()
```


To convert date-time character strings, We'll use **mutate_at** to convert the character-type date-time values back to the needed format.  
```{r}
# Define character to date-time converstion (from lubridate) in a function.
input_function <- function(x, na.rm=FALSE) (ymd_hms(x))

# Define Columns which need to converted from character to date-time.
input_columns <- c("order_purchase_timestamp",
                  "order_approved_at",
                  "order_delivered_carrier_date",
                  "order_delivered_customer_date",
                  "order_estimated_delivery_date")


# Use above defined inputs to mutate the select columns
df_orders_dt <- df_orders %>% 
  mutate_at(.vars = input_columns, 
            .funs = input_function)

#Inspect Conversion
df_orders_dt %>% glimpse()
```

I noticed there is a **order_status** variable. I'm curious what the different status conditions are, and their distribution. We'll use **Hmisc::describe** to look deeper at that variable.
```{r}
df_orders %>% select(order_status) %>%  describe()
```
So 97% of all statuses are that the order was delivered. I suppose this is a very useful value from order to delivery, as the status might change. But looking at historical data, nearly all packages are delivered. Not much information provided by this variable. 

## Location Code Conversion
Some location code information has lost information. Some city zip codes and region codes have leading zero values. When they are imported into this notebook, they are imported as integers, and these leading zeros get dropped. But these codes function as strings. Therefore, we need to convert these codes back to strings and ensure any dropped leading zeros are reinserted.  

The codes that need to be adjusted are:
* Customer Zip
* Seller Zip
* Geo Location Zip
* Municipal code (From Brazilian Population Data)

First we develop a function that takes a dataframe and column name. 

### Function 
```{r}
fun_pad_zero <- function(df, col_name){
  
  col_name = enquo(col_name)
  
  # Determine Max Code Length
  max_len <-df %>% 
  mutate(!!quo_name(col_name) := as.character(!!col_name), 
         code_len = str_length(!!col_name)) %>%
  summarise(max = max(code_len)) %>%
  as.integer()

  # Pad Zeros
  df <- df %>% 
    mutate(!!quo_name(col_name) := str_pad(string = !!col_name,
                                           width = max_len, 
                                           side = "left", 
                                           pad = 0))
  
  return(df)
}
```

### Convert
```{r echo=FALSE, results='hide'}
df_geo_loc<- fun_pad_zero(df = df_geo_loc,
             col_name = geolocation_zip_code_prefix)

df_cust<- fun_pad_zero(df = df_cust,
             col_name = customer_zip_code_prefix)

df_sellers<- fun_pad_zero(df = df_sellers,
             col_name = seller_zip_code_prefix)

df_brazil_pop<- fun_pad_zero(df = df_brazil_pop,
             col_name = cod_munic)
```

Evaluate zero padding worked.
```{r}
df_geo_loc %>% glimpse()

df_cust %>% glimpse()

df_sellers %>% glimpse()

df_brazil_pop %>% glimpse()
```

## Convert Population State Code to Character
The **cod_uf** variable in the Brazil populatin dataframe functions like a character variable. To correclty use it, we need to convert it to character.
```{r}
df_brazil_pop <- df_brazil_pop %>% 
  mutate(cod_uf = as.character(cod_uf))

df_brazil_pop %>% glimpse()
```


### Customer Zip
```{r}
df_cust<- fun_pad_zero(df = df_cust,
             col_name = customer_zip_code_prefix)

df_cust %>% glimpse()
```


```{r}

```


# Sales Breakdown By Categories
The following section will break down the total sales by certain factors. This information is meant to provide some basic descriptive information.

## Product Category
There are 71 number of unique categories. 
```{r}
df_products_translate %>% 
  select(product_category_name_english) %>% 
  describe()
```

This is an unweildy number of unique categories. Lets see what the frequency of each category being used is by sale. Maybe we can drop some. Otherwise, maybe we can group them.

```{r}
category_by_sales <- df_order_items %>% 
  left_join(df_products_translate, by = "product_id") %>% 
  select(product_id, price, product_category_name_english) %>% 
  group_by(product_category_name_english) %>% 
  summarise(total_sales = sum(price)) %>% 
  ungroup() %>% 
  arrange(desc(total_sales)) %>% 
  mutate(percent_total = round((total_sales/sum(total_sales) * 100),2))

category_by_sales
```

Note, the top 20 categories account for 84.02% of sales. While this is a large percentage, I do not think it is dominant enough to just drop other categories. Instead, a select number of superceding categories should be developed.
```{r}
category_by_sales %>% head(20) %>%
  select(-total_sales) %>% 
  summarise(n = sum(percent_total))
```

## Product Id
We want to see what the highest selling items were, and what category they fall into.
```{r}
df_order_items %>% 
  group_by(product_id) %>% 
  summarise(total_sales = sum(price)) %>% 
  ungroup() %>% 
  arrange(desc(total_sales)) %>% 
  mutate(percent_total = round((total_sales/sum(total_sales) * 100),2)) %>% 
  left_join(df_products_translate %>% 
              select(product_id, product_category_name_english), 
            by = "product_id") 
```

## State - Raw
We wil now see what states spend the most.
```{r}
df_total_sales_states <- df_order_pay %>% 
  left_join(df_orders, "order_id") %>% 
  left_join(df_cust, "customer_id") %>% 
  select(payment_value, customer_state) %>% 
  group_by(customer_state) %>% 
  summarise(total_sales = sum(payment_value)) %>% 
  arrange(desc(total_sales)) %>% 
  mutate(total_sales_percent = round((total_sales/sum(total_sales) * 100),1))

df_total_sales_states
```

## State - Population Adjusted
I now want to see what state spends the most per person.

State Population projections from the Brazilian Institute of Geography and Statistics were found here [https://www.ibge.gov.br/en/statistics/social/population/18448-estimates-of-resident-population-for-municipalities-and-federation-units.html?=&t=resultados](https://www.ibge.gov.br/en/statistics/social/population/18448-estimates-of-resident-population-for-municipalities-and-federation-units.html?=&t=resultados). 

I will use these projections to calculate sales per population. First, I need to translate Brazil's population data from to the state level. The population data imported was documented at the municipal district level.

```{r}
df_brazil_pop_state <- df_brazil_pop %>% 
  group_by(uf) %>%
  summarise(pop_estimate = sum(populacao_estimada))
```

Now we join the state population data to the state sales data. Then we calculate money spent per 1K people.
```{r}
df_total_sales_states %>% 
  left_join(df_brazil_pop_state, by = c("customer_state" = "uf")) %>% 
  mutate(sales_per_k = round(1000*total_sales / pop_estimate,1)) %>% 
  arrange(desc(sales_per_k))
```

Nothing generally unexpected here. At first glance, the more prosperous states spend more per person. Note, DF is the Federal District aka Brasilia. So while small (2.2% total sales), it is the capital.

# Product Categories
As previously shown, there are 71 unique categories.
```{r}
df_products_translate %>% select(product_category_name_english) %>% describe()
```

This is may be too many categories to make a useful feature in any modeling. So if possible, it would be worth creating a smaller number of encompassing categories. To do so, we need to create a list of the existing categories.
```{r}
df_product_cat <- df_products_translate %>% 
  select(product_category_name_english) %>% 
  unique() %>% 
  arrange(product_category_name_english)

df_product_cat
```



# Logisic Regression 


# For Later
Now we can calculate the number of days between times. The datetime values we have all contain the exact time of day an action occured. But we don't want to deal with partial days. That is not how the customer would think about it. We only care about the day the action occurred, and the number of days in between.  
```{r}
df_orders_dt_1 <- df_orders_dt %>% 
  mutate(days_order_to_delivery = difftime(as_date(order_delivered_customer_date),
                                           as_date(order_purchase_timestamp),
                                           units = "days"))
df_orders_dt_1 %>% 
  ggplot(aes(x = days_order_to_delivery)) +
  geom_histogram()
```