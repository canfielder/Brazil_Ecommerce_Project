---
title: "Brazilian Ecommerce - EDA"
author: "Evan Canfield"
date: "3/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import
## Libraries
```{r}
if (!require(pacman)) {install.packages('pacman')} 
p_load(
    janitor
  , lubridate
  , Hmisc
  , skimr
  , tidyverse
)
```

## Data
```{r}
# Olist Dataset
df_cust <- read.csv(file = "data/olist_customers_dataset.csv", stringsAsFactors = FALSE)
df_geo_loc <- read.csv(file = "data/olist_geolocation_dataset.csv", stringsAsFactors = FALSE)
df_order_items <- read.csv(file = "data/olist_order_items_dataset.csv", stringsAsFactors = FALSE)
df_order_pay <- read.csv(file = "data/olist_order_payments_dataset.csv", stringsAsFactors = FALSE)
df_order_review <- read.csv(file = "data/olist_order_reviews_dataset.csv", stringsAsFactors = FALSE)
df_orders <- read.csv(file = "data/olist_orders_dataset.csv", stringsAsFactors = FALSE)
df_products <- read.csv(file = "data/olist_products_dataset.csv", stringsAsFactors = FALSE)
df_sellers <- read.csv(file = "data/olist_sellers_dataset.csv", stringsAsFactors = FALSE)
df_translation <- read.csv(file = "data/product_category_name_translation.csv", stringsAsFactors = FALSE)

# Brazillian Population Data
df_brazil_pop <- read.csv(file = "data/population_estimates_dou_2019.csv", stringsAsFactors = FALSE)
```

### Clean Names
The following function converts column names to snake case.
```{r}
clean_names_func <- function(df){
  df <- df %>% clean_names(case = "snake")
  return (df)
}

df_cust <- clean_names_func(df_cust)
df_geo_loc <- clean_names_func(df_geo_loc)
df_order_items <- clean_names_func(df_order_items)
df_order_pay <- clean_names_func(df_order_pay)
df_orders <- clean_names_func(df_orders)
df_products <- clean_names_func(df_products)
df_sellers <- clean_names_func(df_sellers)
df_translation <- clean_names_func(df_translation)
df_brazil_pop <- clean_names_func(df_brazil_pop)
```

# Inspect Data
```{r}
df_cust %>% glimpse()
```

```{r}
df_geo_loc %>% glimpse()
```

```{r}
df_order_items %>% glimpse()
```

```{r}
df_order_pay %>% glimpse()
```

```{r}
df_order_review %>% glimpse()
```

```{r}
df_orders %>% glimpse()
```

```{r}
df_products %>% glimpse()
```

```{r}
df_sellers %>% glimpse()
```

```{r}
df_translation %>% glimpse()
```

```{r}
df_brazil_pop %>% glimpse()
```

# Data Processing
There are several areas where we need to perform processing on the data before we can start to analyze.

## Typos
There are two typos in the column names for Products (may be an issue with translation/symbol conversion.) 
```{r}
df_products_corr <- df_products %>% 
  rename(product_name_length = product_name_lenght,
         product_description_length = product_description_lenght)
```

## Translate
We need to translate the product category name from Portuguese to English for greater understanding. The dataset provides a translation code dataset
```{r}
df_products_translate <- df_products_corr %>% 
  left_join(y = df_translation,
            by = c("product_category_name" = "i_product_category_name")) %>% 
  select(product_id, product_category_name, product_category_name_english, everything()) %>% 
  select(-product_category_name) %>% 
  rename(product_category_name_eng = product_category_name_english)

df_products_translate %>% glimpse()
```

Now that we've translated the data, we'll take a deeper look at the contents of the dataframe.
```{r}
#df_products_translate %>% describe()
```

## Product IDs - Shorten
The product ID tags are very long, and difficult to work with. For other id tags, this isn't a problem, because they will only be used for connecting tables. But the product id may be displayed on charts and tables. 

I want to see if I can shorten it, while maintaining the unique tag nature. There are 32,951 unique product id tags.
```{r}
prod_unique <- df_products_translate %>% 
  select(product_id) %>% 
  n_distinct()

paste0("The number of unique product ids: ", prod_unique)
```
The shortest product string in the product id column is 32 characters. So is the maximum. In actuality, all product id tags are 32 characters.
```{r}
min_prod_id_string <- min(nchar(df_products_translate$product_id))

paste0("The minimum length of a product id string is ", min_prod_id_string, " characters.")
```


If we shorten the tag, we need to determine that we still have unique ids. The following code crops the product id by **n** characters, from the right. It then checks against the known number of unique product ids. The process continues until enough characters have been cropped to create some duplicate id values. 
```{r}
n <- prod_unique
x = min_prod_id_string

while (TRUE){
  if (n == prod_unique){
    n <- df_products_translate %>% 
    select(product_id) %>% 
    mutate(product_id = str_sub(product_id, start = -x)) %>% 
    n_distinct()
    
    #print(paste0("Unique values: ",n))
    #print(paste0("Number of digits:", x))
    #print(paste0("***"))
    x = x-1
  }
    else{
      output_response <- paste0("Shortest string to maintain unique product_id: ", x + 2, " characters.")
      return(print(output_response))
    }
}
```

So, we can shorten product id to only 8 characters, a much more manageable string to view in tables and charts. We need to make this change at all product_id instances.
```{r}
df_products_translate <- df_products_translate %>% 
  mutate(product_id = str_sub(product_id, start = -x-2))

df_order_items <- df_order_items %>% 
  mutate(product_id = str_sub(product_id, start = -x-2))
```

 
## Convert Time
shipping times will be an important variable for further analysis. We need to convert date-time values, which were imported as characters, into Posix date-time values. Once this is complete, we can then easily  

Let's take a quick look at the table.
```{r}
df_orders %>% 
  head()
```

Now lets look for missing data. Lucky for us there is no missing data in this table.
```{r}
df_orders %>%   skim()
```


To convert date-time character strings, We'll use **mutate_at** to convert the character-type date-time values back to the needed format.  
```{r}
# Define character to date-time converstion (from lubridate) in a function.
input_function <- function(x, na.rm=FALSE) (ymd_hms(x))

# Define Columns which need to converted from character to date-time.
input_columns <- c("order_purchase_timestamp",
                  "order_approved_at",
                  "order_delivered_carrier_date",
                  "order_delivered_customer_date",
                  "order_estimated_delivery_date")


# Use above defined inputs to mutate the select columns
df_orders <- df_orders %>% 
  mutate_at(.vars = input_columns, 
            .funs = input_function)

#Inspect Conversion
df_orders %>% glimpse()
```

I noticed there is a **order_status** variable. I'm curious what the different status conditions are, and their distribution. We'll use **Hmisc::describe** to look deeper at that variable.
```{r}
df_orders %>% select(order_status) %>%  describe()
```
So 97% of all statuses are that the order was delivered. I suppose this is a very useful value from order to delivery, as the status might change. But looking at historical data, nearly all packages are delivered. Not much information provided by this variable. 

## Location Code Conversion
Some location code information has lost information. Some city zip codes and region codes have leading zero values. When they are imported into this notebook, they are imported as integers, and these leading zeros get dropped. But these codes function as strings. Therefore, we need to convert these codes back to strings and ensure any dropped leading zeros are reinserted.  

The codes that need to be adjusted are:
* Customer Zip
* Seller Zip
* Geo Location Zip
* Municipal code (From Brazilian Population Data)

First we develop a function that takes a dataframe and column name. 

### Function 
```{r}
fun_pad_zero <- function(df, col_name){
  
  col_name = enquo(col_name)
  
  # Determine Max Code Length
  max_len <-df %>% 
  mutate(!!quo_name(col_name) := as.character(!!col_name), 
         code_len = str_length(!!col_name)) %>%
  summarise(max = max(code_len)) %>%
  as.integer()

  # Pad Zeros
  df <- df %>% 
    mutate(!!quo_name(col_name) := str_pad(string = !!col_name,
                                           width = max_len, 
                                           side = "left", 
                                           pad = 0))
  
  return(df)
}
```

### Convert
```{r echo=FALSE, results='hide'}
df_geo_loc<- fun_pad_zero(df = df_geo_loc,
             col_name = geolocation_zip_code_prefix)

df_cust<- fun_pad_zero(df = df_cust,
             col_name = customer_zip_code_prefix)

df_sellers<- fun_pad_zero(df = df_sellers,
             col_name = seller_zip_code_prefix)

df_brazil_pop<- fun_pad_zero(df = df_brazil_pop,
             col_name = cod_munic)
```

Evaluate zero padding worked.
```{r}
df_geo_loc %>% glimpse()

df_cust %>% glimpse()

df_sellers %>% glimpse()

df_brazil_pop %>% glimpse()
```

## Convert Population State Code to Character
The **cod_uf** variable in the Brazil population dataframe functions like a character variable. To correctly use it, we need to convert it to character.
```{r}
df_brazil_pop <- df_brazil_pop %>% 
  mutate(cod_uf = as.character(cod_uf))

df_brazil_pop %>% glimpse()
```


### Customer Zip
```{r}
df_cust<- fun_pad_zero(df = df_cust,
             col_name = customer_zip_code_prefix)

df_cust %>% glimpse()
```


```{r}

```


# Sales Breakdown By Categories
The following section will break down the total sales by certain factors. This information is meant to provide some basic descriptive information.

## Product Category
There are 71 number of unique categories. 
```{r}
df_products_translate %>% 
  select(product_category_name_eng) %>% 
  describe()
```

This is an unwieldy number of unique categories. Lets see what the frequency of each category being used is by sale. Maybe we can drop some. Otherwise, maybe we can group them.

```{r}
category_by_sales <- df_order_items %>% 
  left_join(df_products_translate, by = "product_id") %>% 
  select(product_id, price, product_category_name_eng) %>% 
  group_by(product_category_name_eng) %>% 
  summarise(total_sales = sum(price)) %>% 
  ungroup() %>% 
  arrange(desc(total_sales)) %>% 
  mutate(percent_total = round((total_sales/sum(total_sales) * 100),2))

category_by_sales
```

Note, the top 20 categories account for 84.02% of sales. While this is a large percentage, I do not think it is dominant enough to just drop other categories. Instead, a select number of superseding categories should be developed.
```{r}
category_by_sales %>% head(20) %>%
  select(-total_sales) %>% 
  summarise(n = sum(percent_total))
```

## Product Id
We want to see what the highest selling items were, and what category they fall into.
```{r}
df_order_items %>% 
  group_by(product_id) %>% 
  summarise(total_sales = sum(price)) %>% 
  ungroup() %>% 
  arrange(desc(total_sales)) %>% 
  mutate(percent_total = round((total_sales/sum(total_sales) * 100),2)) %>% 
  left_join(df_products_translate %>% 
              select(product_id, product_category_name_eng), 
            by = "product_id") 
```

## State - Raw
We will now see what states spend the most.
```{r}
df_total_sales_states <- df_order_pay %>% 
  left_join(df_orders, "order_id") %>% 
  left_join(df_cust, "customer_id") %>% 
  select(payment_value, customer_state) %>% 
  group_by(customer_state) %>% 
  summarise(total_sales = sum(payment_value)) %>% 
  arrange(desc(total_sales)) %>% 
  mutate(total_sales_percent = round((total_sales/sum(total_sales) * 100),1))

df_total_sales_states
```

## State - Population Adjusted
I now want to see what state spends the most per person.

State Population projections from the Brazilian Institute of Geography and Statistics were found here [https://www.ibge.gov.br/en/statistics/social/population/18448-estimates-of-resident-population-for-municipalities-and-federation-units.html?=&t=resultados](https://www.ibge.gov.br/en/statistics/social/population/18448-estimates-of-resident-population-for-municipalities-and-federation-units.html?=&t=resultados). 

I will use these projections to calculate sales per population. First, I need to translate Brazil's population data from to the state level. The population data imported was documented at the municipal district level.

```{r}
df_brazil_pop_state <- df_brazil_pop %>% 
  group_by(uf) %>%
  summarise(pop_estimate = sum(populacao_estimada))
```

Now we join the state population data to the state sales data. Then we calculate money spent per 1K people.
```{r}
df_total_sales_states %>% 
  left_join(df_brazil_pop_state, by = c("customer_state" = "uf")) %>% 
  mutate(sales_per_k = round(1000*total_sales / pop_estimate,1)) %>% 
  arrange(desc(sales_per_k))
```

Nothing generally unexpected here. At first glance, the more prosperous states spend more per person. Note, DF is the Federal District aka Brasilia. So while small (2.2% total sales), it is the capital.

# Product Categories
As previously shown, there are 71 unique categories.
```{r}
df_products_translate %>% select(product_category_name_eng) %>% describe()
```

This is may be too many categories to make a useful feature in any modeling. So if possible, it would be worth creating a smaller number of encompassing categories. To do so, we need to create a list of the existing categories.
```{r}
df_product_cat <- df_products_translate %>% 
  select(product_category_name_eng) %>% 
  unique() %>% 
  arrange(product_category_name_eng)

df_product_cat
```



# Logistic Regression 
## Determine Return Customers
We want to see if we can determine what factors impact whether a consumer is a return consumer or a one-time user. 

We start by determining the amount of orders each customer made. We will initially be using the Orders dataframe. This dataframe lists each order by **customer_id**. But each unique customer can have multiple **customer_id** values. That is why there is a **customer_unique_id** value in the Customer dataframe. We need to join that dataframe in order to evaluate unique customers.

Then, we use the **add_count** function to count the number of times **customer_id** occurs.
```{r}
df_orders <- df_orders %>% 
  left_join(df_cust, by = "customer_id") %>% 
  add_count(customer_unique_id) %>% 
  rename(order_count = n)
```

With the number of orders each unique customer, I can label which customers are repeat, and which are not.
```{r}
df_orders <- df_orders %>% 
  mutate(repeat_cust = if_else(condition = order_count > 1, 1, 0)) 
```

## Create Dataset - Customer Based
Now that we have defined our dependent variable, we need to create the dataset. We care going to make this a customer based dataset, where each observation represents a customer.Therefore, we will need aggregate values for each customer. The following will be used as features:

* Spent - price, avg
* Spent - freight, avg
* Shipping times, avg


# Customer Segmentation
## Create Dataset
To create the input dataset for custermer segmentaion, we are going to look at the following variables: 

* Spent (Price), avg
* Spent (Freight), avg
* Payment Installments, avg
* Review score, avg (Check complete enough)
* Shipping time, avg (days)
* Distance between customer/seller (maybe)

```{r}
df_cust %>% glimpse()

df_order_items %>% glimpse() 

df_order_pay %>% glimpse()  

df_order_review %>% glimpse()

df_orders %>% glimpse()
```


First, we slim down the order dataframe to useful variables. I also notice a small amount of NAs in the order data. This is most likely due to canceled orders. It is a small enough percentage, and the missing data cannot be imputed, so we will drop this data.
```{r}
df_cust_seg_1 <- df_orders %>% 
  select(order_id, customer_id, customer_unique_id, 
         order_purchase_timestamp, order_approved_at, order_delivered_customer_date) %>% 
  drop_na()
```

Furthermore, we can then calculate shipping and acceptance time with the available date-time variables. 
```{r}
# Calculate Shipping Time - Days
df_cust_seg_2 <- df_cust_seg_1 %>% 
  mutate(days_to_deliver = difftime(as.Date(order_delivered_customer_date),
                                    as_date(order_purchase_timestamp),
                                     units = "days"),
         days_to_deliver = as.integer(days_to_deliver))

# Calculate Acceptance Time - Minutes
df_cust_seg_3 <- df_cust_seg_2 %>% 
    mutate(mins_to_accept = difftime(order_approved_at,
                                    order_purchase_timestamp,
                                     units = "mins"),
  mins_to_accept = as.integer(mins_to_accept)
    )

# Drop Variables No Longer Needed
df_cust_seg_4 <- df_cust_seg_3 %>% 
  select(order_id, customer_unique_id,
         days_to_deliver, mins_to_accept)

# Insepct Data
df_cust_seg_4 %>% glimpse()
```

Now we will connect the review data to see if there is a complete enough dataset to use that information. 
```{r}

```



# For Later
Now we can calculate the number of days between times. The date-time values we have all contain the exact time of day an action occurred. But we don't want to deal with partial days. That is not how the customer would think about it. We only care about the day the action occurred, and the number of days in between.  
```{r}
df_orders_dt_1 <- df_orders_dt %>% 
  mutate(days_order_to_delivery = difftime(as_date(order_delivered_customer_date),
                                           as_date(order_purchase_timestamp),
                                           units = "days"))
df_orders_dt_1 %>% 
  ggplot(aes(x = days_order_to_delivery)) +
  geom_histogram()
```